
### Problem Statement
The goal of this project is to generate video captions using a sequential-to-sequential model. Given an input video, the model generates a stream of captions that describe the actions in the video. This is achieved using Recurrent Neural Networks.

### Requirements
To run the code, the following requirements must be met:
- Python
- Cuda (GPU)
- torch
- scipy
- numpy
- pandas
- pickle

### Dataset
The model is trained and tested using a dataset provided by the professor. The dataset consists of 1450 videos for training and 100 videos for testing. Additionally, there are training and testing label .json files that provide labels for training the model.

### Program Data Structures
The seq2seq model requires specific data structures. These include:
- word_dict: A dictionary that contains the number of occurrences of each word in the training label file. Words with a frequency less than 4 are ignored.
- w2i: A word-to-index mapping for the words in the vocabulary.
- i2w: An index-to-word reverse mapping for the vocabulary.

### Model
The model consists of two layers: an encoder and a decoder. Both layers use GRU (Gated Recurrent Units) for processing. GRU is chosen over LSTM due to its faster execution and comparable accuracy for smaller sequences.

The encoder processes the video and encodes it into the necessary format. The decoder segments the captions based on the beginning and ending tokens and generates the actual words.

### Attention Layer
An attention layer is used to allow the model to focus on different sections of inputs at each decoding time step. The structure of the attention layer is based on the paper titled "Attention-based convolution neural network for semantic relation extraction" by Shen and Huang.

### Schedule Sampling
During inference, the model replaces unknown previous tokens with tokens generated by itself. This can lead to errors due to the discrepancy between training and inference. To mitigate this, the model is trained by feeding either the ground truth or the last time step's output as input.

### Model Parameters and Evaluation
The model is trained with the following parameters:
- Epochs: 100
- Learning Rate: 0.0001
- Batch Size: 128
- Hidden Layers: 512
- Optimizer: Adam Optimizer
- Dropout: 0.3
- Teacher Learning Ratio: 0.7
- Vocabulary Size: n > 4

The loss of the model in the last epoch is 1.4123.

The BLEU score of the model is 0.699 for the provided test data.
